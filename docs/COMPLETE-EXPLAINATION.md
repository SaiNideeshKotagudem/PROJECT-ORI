# ğŸ§¬ RNA 3D Folding Prediction: Multi-Modal Deep Learning Architecture

> A modular PyTorch-based pipeline to predict RNA tertiary structure (x, y, z atomic coordinates) from primary sequence, secondary structure, base pairing probabilities, and text-based annotations.
>
> This architecture is designed for the **Stanford Ribonanza Challenge**, integrating GNNs, Transformers, multi-modal fusion, and DeepSpeed acceleration.

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ sequence_encoder.py            # Transformer-based encoder for nucleotide sequences
â”‚   â”œâ”€â”€ description_encoder.py         # Transformer-based encoder for description strings
â”‚   â”œâ”€â”€ secondary_structure_encoder.py # Message-passing GNN for base-pair graphs
â”‚   â”œâ”€â”€ bpp_graph_encoder.py           # Attention-based encoder for BPP graph
â”‚   â”œâ”€â”€ feature_fusion.py              # Cross-attention + projection-based fusion
â”‚   â”œâ”€â”€ final_model.py                 # End-to-end architecture integrating all components
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset.py                     # Custom dataset loader for sequence/label CSVs
â”‚   â”œâ”€â”€ graph_utils.py                 # Utilities for edge_index, masking, etc.
â”œâ”€â”€ train.py                           # Training script (DeepSpeed integrated)
|___predict.py # Evaluation and prediction on test set
â”œâ”€â”€ utils.py                           # General utility functions
â”œâ”€â”€ config.py                          # Model and training configuration parameters
â””â”€â”€ README.md                          # This file
```

---

## ğŸ§  Model Architecture Overview

The architecture predicts the 3D coordinates of RNA residues given multiple input modalities:

1. **Primary Sequence** (A, U, G, C) â†’ Transformer encoder
2. **Secondary Structure** â†’ Message-Passing GNN
3. **Base Pairing Probability Matrix (BPP)** â†’ Graph Attention Encoder
4. **Freeform Description Text** â†’ Transformer text encoder
5. **Fusion** â†’ Cross-attention followed by projection
6. **Output Head** â†’ Fully connected network to regress 3D coordinates

---

## ğŸ”¢ Input Modalities

### ğŸ§¬ 1. Primary Sequence

* Format: One-hot encoding of RNA sequence (A, U, G, C) â†’ shape `[B, L, 4]`
* Embedded via a learnable linear projection to `d_model=128`
* Positional encoding added using sinusoidal formula
* Passed through `N=3` layers of `nn.TransformerEncoderLayer` (PyTorch-native)

  * `d_model=128`, `nhead=8`, `dim_feedforward=512`, dropout=0.1

### ğŸ““ 2. Description Encoder (Text Metadata)

* Input: Freeform strings (e.g., experimental conditions, tags)
* Tokenized as ASCII byte-level character IDs âˆˆ \[0, 127]
* Embedded using `nn.Embedding(128, 128)`
* Positional encoding (sinusoidal) added
* Two-layer Transformer (`nhead=4`, `ff=256`)
* Final representation is globally pooled (mean) and projected to `[B, 128]`

### ğŸ”— 3. Secondary Structure Encoder

* Graph-based input: RNA secondary structure edges â†’ `edge_index` format
* Node features: One-hot sequence embedding
* Implements a 2-layer GNN with degree-normalized message passing:

  ```python
  m_i = Î£_j âˆˆ N(i) (W1 Â· x_j) / deg(i)
  x_i = ReLU(W2 Â· m_i)
  ```
* Activation: ReLU; Output dim: `[B, L, 128]`
* Edge dropout applied for robustness

### ğŸ” 4. Base Pairing Probability (BPP) Graph Encoder

* Input: Sparse upper-triangular matrix with BPP weights
* Encoded as graph: nodes are nucleotides, edges carry BPP weights
* Uses a **Graph Attention Mechanism**:

  * Keys, Queries, Values computed per node
  * Attention weights scaled by edge BPP probability
  * Final node representations refined by attention-weighted sum over neighbors
  * Multi-head (optional), LayerNorm, dropout used
* Output: `[B, L, 128]`

---

## ğŸ§¬ Feature Fusion: Cross-Modality Attention

The sequence embeddings are contextually fused with the description vector:

```python
desc_expanded = repeat(desc_vec, L, axis=1)
attn_output, _ = MultiHeadAttention(query=seq_embed,
                                    key=desc_expanded,
                                    value=desc_expanded)
```

* Cross-attention produces sequence-aware conditioning on description.
* Attention output is concatenated with original embeddings â†’ `[B, L, 256]`
* Final fusion: `LayerNorm â†’ Linear(256 â†’ 256) â†’ GELU`

---

## ğŸ¯ Prediction Head

```python
Fused Features â†’ MLP Head â†’ Regress (x, y, z)
```

* Head:

  * Linear(256 â†’ 512) â†’ ReLU
  * Linear(512 â†’ 256) â†’ ReLU
  * Linear(256 â†’ 3)

Optionally, auxiliary heads can be added for:

* Residue identity prediction
* Base-pair classification
* Structure confidence estimation

---

## ğŸ§ª Loss Function

Multi-objective loss with configurable weighting:

| Component       | Description                              | Default Weight |
| --------------- | ---------------------------------------- | -------------- |
| `MSE_Loss`      | Mean squared error over predicted coords | `1.0`          |
| `Resid_ID_Loss` | MSE or BCE over residue ID positions     | `0.5`          |
| `Resname_Loss`  | CrossEntropy on predicted base labels    | `0.5`          |
| `Coverage_Loss` | % of residues with valid prediction      | `1.0`          |

Total loss:

```python
loss = w1 * coord_loss + w2 * resid_loss + w3 * resname_loss + w4 * coverage_loss
```

---

## âš¡ Training Setup (DeepSpeed Enabled)

```bash
deepspeed train.py --deepspeed config/ds_config.json
```

### âœ… DeepSpeed Features:

* ZeRO Stage 1 support for optimizer state sharding
* Automatic FP16 mixed-precision
* FusedAdam optimizer (CUDA-native)

### ğŸ”§ Key Training Config:

```python
batch_size      = 16
learning_rate   = 1e-3
optimizer       = FusedAdam
epochs          = 25+
warmup_steps    = 500
grad_clip       = 1.0
```

---

## ğŸ”¬ Evaluation

* Coordinate MAE, RMSE, Pearson r per-axis
* Optional: contact map agreement, torsion angle distance
* 3D structure visualization tools with PyMOL or Matplotlib

---

## ğŸ§ª Dataset Format

### `train_sequences.csv`

\| id     | sequence     | description    | ...
\|--------|--------------|----------------|

### `train_labels.csv`

| id | resid | x | y | z |
| -- | ----- | - | - | - |

* Join on `(id, resid)`
* All coordinates in Angstroms (Ã…)
* Sequences are padded to fixed maximum length (`max_len=512`)

---

## ğŸ› ï¸ Future Work

* Integrate **SE(3)-equivariant GNNs** (e.g. EGN, SE3-Transformer)
* Use **graph pooling** for long sequences
* Add **attention visualizations** for interpretability
* Pretrain with self-supervised contrastive learning

---

## ğŸ“š References

* [SE(3)-Transformer: Equivariant Attention for 3D Data](https://arxiv.org/abs/2006.10503)
* [Graph Attention Networks (GAT)](https://arxiv.org/abs/1710.10903)
* [Transformer Architecture (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762)

---

## ğŸ§‘â€ğŸ’» Author

**Sai Nideesh Kotagudem**
Email: [sainideeshkotagudem@gmail.com](mailto:sainideeshkotagudem@gmail.com)
GitHub: [github.com/SaiNideeshKotagudem](https://github.com/SaiNideeshKotagudem)
